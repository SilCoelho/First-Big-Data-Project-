#!/usr/bin/env python
# coding: utf-8

# ## **Suicide Within One Year After Discharge Among Patients Diagnosed With a Mental Disorder**
# 
# 

# ![230306_r41939.webp](data:image/webp;base64,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)

# ### Stages of the project
# 
# ###This project is divided into two stages, as modules:
# 
# 1. **Module 1. Advanced Data Analytics**
# 
# 3. **Module 3. Big Data Storage and Processing**

# https://www.newyorker.com/magazine/2023/03/06/can-ai-treat-mental-illness

# ## 1.1 Planning
# 
# This project implements the following frameworks in order to have an appropiate planning and execution, delivering meaningful and trustworthy insights:
# 
# 
# *  **Introduction**: Provide an introductory section outlining the background, significance, and objectives of your research. Clearly state the research question and goals.
# 
# **Data Gathering**:
# 
# * **Data Sources**: Identify data sources that can provide relevant information about individualsâ€™ mental health, behavior, and potential risk factors. These sources may include:
# Social media posts and interactions
# Medical records (with consent and following ethical guidelines)
# Crisis Hotline Call Logs
# Online forums and communities
# Mobile app usage data (with user consent)
# Demographic and socioeconomic data
# 
# *  **Data collection**: Detail the sources of data, data acquisition methods, and the dataset's composition. Describe how you obtained and prepared the data, including ethical considerations.
# 
# **Data Analysis**:
# 
# * **Data preprocessing**: Clean, normalize, and preprocess collected data to make it suitable for analysis.
# 
# * **Feature Extraction**: Extract relevant features from data that can serve as potential risk factors. This can include text sentiment analysis, social network analysis, and more.
# 
# * **Model selection**: Decide on appropriate models or machine learning techniques to identify risk factors. Neural networks, natural language processing models (e.g. BERT for sentiment analysis) and statistical models could be considered.
# 
# * **Training and evaluation**: Train your models using historical data with known outcomes (e.g. suicide cases) and evaluate their performance using metrics such as precision, recall, and F1 score.
# 
# **Real-Time Risk Factor Identification**:
# 
# * **Streaming Data Processing**: Design a real-time data processing system that can continuously ingest and process new data as it becomes available. Technologies like Apache Kafka, Apache Flink, or Apache Spark Streaming may be useful.
# 
# * **Model Deployment**: Deploy your trained models to the real-time processing system. These models should continuously analyze incoming data to identify potential risk factors.
# 
# * **Alerting and Notification**: Set up alerts or notifications to flag potential risk factors in real time so that appropriate intervention can be initiated promptly.
# 
# 
#   **Intervention and Tackling the Situation**:
# 
#  * **Crisis Response System**: Implement a system that can provide immediate support to individuals identified as at-risk. This might include automated messages, contacting crisis hotlines, or notifying relevant authorities.
# 
# * **Human Review**: Always involve trained mental health professionals to review and validate the risk factors identified by the system.
# 
# * **Privacy and Ethics**: Ensure that your system respects user privacy and adheres to ethical guidelines, including obtaining informed consent for data collection and handling.
# 
# **Monitoring and Iteraction**:
# 
# Continuously monitor and evaluate the performance of your system, including the accuracy of risk factor identification and the effectiveness of intervention strategies.
# Use feedback from mental health professionals and individuals in crisis to improve your system iteratively.
# 
#  **Ethical Considerations**:
# 
#  Be mindful of ethical considerations, including data privacy, consent, bias in models, and the potential for false positives and negatives.
# 
#  **Collaboration**:
# 
#  Collaborate with mental health experts, ethicists, and legal professionals to ensure that your project is conducted in a responsible and ethical manner.
# 
#  **Legal and Regulatory Compliance**:
# 
# Ensure compliance with all relevant laws and regulations regarding data privacy and mental health data handling.
# 
# 
#   **Conclusion and Discussion**: In this part, we will discuss the results that we have in the visualization and exploration part. We will conclude how people's emotion change during the time period.
# 
#   **Git Versioning**: Will be the  project repository, public access.
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 

# ### 1.1.1. Research Questions
# 
# There are some research questions formulated for this research. In the following sections of this notebook, it will be asessed if it is possible to answer those questions with the dataset, or we need a different approach:
# 
# These comparisons will be based on data collection from people who committed suicide after being discharged from hospital.
# 
# 1. Are there statistically significant differences in suicide risk between patients with various types of mental disorders, and can these differences be used to inform targeted intervention strategies?
# 
# 2. What are the main factors, including demographic and clinical variables, that contribute to the risk of suicide within a year after discharge among patients diagnosed with mental disorders?
# 
# 3. How do different features or indicators related to mental health, such as depression severity, treatment adherence, or follow-up care, impact the risk of suicide after discharge?
# 
# 4. Can machine learning models effectively predict the risk of suicide among patients with specific mental disorders, and if so, which models perform best in this context?
# 

# #**1. Module 1. Advanced Data Analytics**

# ###**Dependencies Installation**

# In[ ]:


get_ipython().system('pip install pandas')
get_ipython().system('pip install numpy')
#!pip install numpy pandas (it is the same)
get_ipython().system('pip install scikit-learn')


# In[ ]:


#[Write about what I am trying to do here]
get_ipython().system('pip install transformers')
get_ipython().system('pip install torch')


# In[ ]:


#importing required libraries
import numpy as np
import pandas as pd                     # data processing, CSV file
# Set the number of max columns and rows that can be visualized:
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

import missingno as msno                  #Visualization of missing values

import matplotlib.pyplot as plt           #visualisation
get_ipython().run_line_magic('matplotlib', 'inline')

import seaborn as sns                     #visualisation
sns.set(color_codes=True)
sns.set(style = 'dark')                   #sns.set(style="darkgrid") also available


# In[ ]:


#[Write about what I am trying to do here]
import os
import codecs
import scipy.stats as stats


# In[ ]:


#[Write about what I am trying to do here]
import cv2


# In[ ]:


#[Write about what I am trying to do here]
from matplotlib.texmanager import TemporaryDirectory
TemporaryDirectory


# In[ ]:


# Import Neural Networks libraries
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers


# In[ ]:


from google.colab import drive
drive.mount("/content/drive", force_remount=True)


# 
# ##**1- Data Collection**
# 
# ##Loading the data into data frame, the dataset is borrowed from website https://stats.oecd.org/

# In[ ]:


# Importing dataset and examining it
# Read in the data
#pd.read_csv("/content/_HEALTH_HCQI_.csv")

data = pd.read_csv("/content/.config/content_HEALTH_HCQI_.csv.csv")
data.head(5)


# In[ ]:


# Loading data into data frame (If the data is in .config folder)

#dataset = pd.read_csv("/content/.config/content_HEALTH_HCQI_.csv", encoding="latin-1")
# encoding in latin-1 do not work good. it is better in utf-8

data = pd.read_csv("/content/.config/content_HEALTH_HCQI_.csv.csv")
data.head(5)


# In[ ]:


#print(data.head())


# ##**2.Familiarizing with the  data:**
# 
# In this step, few dataframe methods are used to look into the data and its features.
# 
# Printing Type of data to identify problem.
# 
# Printing the data types to confirm if there problems in data.

# In[ ]:


# To display the botton 5 rows
data.tail(5)


# In[ ]:


# [Write about what I am trying to do here and why...]
#data['Indicator'] = 4933


# In[ ]:


#Transpose the dataframe (columns become index, index becomes columns)
data.T


# ### Checking rows and columns

# In[ ]:


#Listing the features of the dataset

data.columns


# CHECKING THE TYPES OF DATA

# In[ ]:


# Print data types of DataFrame
data.dtypes


# In[ ]:


# Checking rows and columns format
data.info()
# everything looks fine so far, no encoding errors, no special characters.


# In[ ]:


#Visualizing the variables that are in the data
data_variables1 = pd.DataFrame(data["Indicator"].unique(), columns=["Indicator"])
data_variables1
# these 4 variables have different meaning and may or may not be of the same nature, or in the same scale or interpretation


# Let's see how many countries are avaialble in the dataset:

# In[ ]:


unique_countries = data['Country'].nunique()
print("Number of unique countries:", unique_countries)


# In[ ]:


data.Country.value_counts()
# some countries report much less data


# In[ ]:


data.Periods.value_counts()
# we have 23 possible periods. Data is missing in more recent and more earlier years


# In[ ]:


data.Indicator.value_counts()
# Suicide is reported as a P-value with lower and higher confidence limits


# In[ ]:


data.Gender.value_counts()


# In[ ]:


data["Age Group"].value_counts()
# Both values mean the same age group


# In[ ]:


data.Value.value_counts()


# In[ ]:


data["Value.1"].value_counts()
# values are not unique, many datapoints have the same value


# In[ ]:


data.Flags.value_counts()
# Flags have no information


# ### Consistency
# 
# To evaluate if the values in two categorical variables represent a pair code-value relationship, we can check if each code has a unique corresponding value and vice versa.
# 
# Also, in this way we can catch errors of spelling, encoding, typos, etc.

# In[ ]:


# Check How many different values of data we have for each pair of variables:
# Pair IND Indicator
data.groupby(['IND', 'Indicator']).nunique()

# it is similar to use the method '.value_counts()', but it gives us more information, as we can understand where and why some values may be missing


# In[ ]:


# Pair COU Country
data.groupby(['COU', 'Country']).nunique()


# In[ ]:


# Pair PER Periods
data.groupby(['PER', 'Periods']).nunique()


# In[ ]:


# Pair GEN Gender
data.groupby(['GEN', 'Gender']).nunique()


# In[ ]:


# Pair AGE	Age Group
data.groupby(['AGE', 'Age Group']).nunique()


# In[ ]:


# Pair VAL Value
data.groupby(['VAL', 'Value']).nunique()


# In[ ]:


# Pair Flag Codes and Flags
data.groupby(['Flag Codes', 'Flags']).nunique()


# In[ ]:


# How many datapoints are in each combination of Value - Indicator - Country?
data[['Country','Periods', 'Indicator', 'Gender', 'Value', 'Value.1']].groupby(['Country', 'Indicator', 'Value']).nunique()


# In[ ]:


# How many datapoints are for each combination of country, indicator, gender and value?
# We are expecting one datapoint for each year, 23 in total.
data[['Country','Periods', 'Indicator', 'Gender', 'Value', 'Value.1']].groupby(['Country', 'Indicator', 'Gender', 'Value']).nunique()


# In[ ]:


# Visualize just a portion of the dataset to understand why "Value.1" values are repeated or missing: (No, they are not missing)
data[(data["COU"]=="CAN") & (data["IND"]=="EXCESCHI") & (data["GEN"]=="F") & (data["VAL"]=="AS_STD_RATIO")] #['Value.1'].value_counts()


# ### Statistical
# 
# Standardized rate and standardized ratio are both statistical measures used to make meaningful comparisons between populations, particularly when comparing groups with different characteristics. However, they are used for different types of data and express different types of relationships:
# 
# Standardized Rate:
# 
#         A standardized rate is used to compare the occurrence of an event or condition, typically a count or rate, between two or more populations while accounting for differences in population composition.
#         It is often used in situations where you want to compare the rates of an event, such as mortality or disease incidence, in different populations, but these populations have different age, gender, or other demographic compositions.
#         Standardized rates are expressed per a standard population, which means that the rates are recalculated as if all populations being compared had the same demographic distribution.
#         The formula for a standardized rate is often as follows: (Observed count or rate / Standard population count or rate) * 100 or *1,000 to express it per a standard population size.
# 
# Standardized Ratio:
# 
#         A standardized ratio is used to compare two quantities, typically ratios, between two or more populations while adjusting for differences in population characteristics.
#         It is often used in situations where you want to compare the relationship between two variables, such as comparing the sex-specific incidence rate of a disease in one population to that in another population.
#         Standardized ratios adjust for differences in the distribution of a specific variable, such as age or gender, to make fair comparisons between populations.
#         The formula for a standardized ratio often involves dividing the observed ratio in the population of interest by the expected ratio based on the standard population's distribution of the characteristic of interest.
# 
# In summary, the key difference is in what they are used for and what they express:
# 
#     Standardized rates compare the occurrence of an event or condition between populations, adjusting for differences in demographics.
#     Standardized ratios compare the relationship between two variables or ratios between populations, adjusting for differences in a specific characteristic (e.g., age, gender) within those populations.
# 
# Both standardized rates and ratios are valuable tools in epidemiology, public health, and social sciences for making valid comparisons between populations with different characteristics. The choice between them depends on the research question and the type of data being analyzed.

# In[ ]:


# Have a statistical description of the dataset (only work with numerical values):
#pata.describe_option

data.describe()


# In[ ]:


# Visualizing the variables that are in the data
data_variables2 = data[['Indicator', 'Value', 'Value.1']].groupby(["Indicator", "Value"]).agg(['count', 'size', 'mean', 'median', 'min', 'max', 'std', 'var'])#.reset_index(name='Count')
data_variables2


# In[ ]:


#Visualizing the variables that are in the data
data_variables3 = data[['Indicator', 'Value', 'Gender', 'Age Group', 'Value.1']].groupby(["Indicator", 'Gender', 'Age Group', "Value"]).agg(['count', 'size', 'mean', 'median', 'min', 'max', 'std', 'var'])#.reset_index(name='Count')
data_variables3


# #### HANDLING MISSING OR NULL VALUES

# In[ ]:


print("Are there any missing values in the dataset ?", data.isnull().values.any())


# In[ ]:


# check missing values
data.isnull()
# Yes, the missing data is the Flags


# In[ ]:


# using notnull()
data.notnull()


# In[ ]:


# Where are the missing or null values?
# sum of missing values: by default axis = 0
data.isnull().sum()


# In[ ]:


# drop missing: drop row contains missing values
# it is inplace = False
data.dropna(how='any').shape


# In[ ]:


# Calculate how the dataframe would change if dropping the missing values
print("Shape: \n", data.dropna().shape, "\n")
print("Count of values: \n", data.dropna().count(), "\n")
print("Missing values: \n", data.dropna().isnull().sum())


# In[ ]:


# Using crosstabs to count number datapoints in order to check where else is missing data:
# notice that here we use the unpivoted dataframe, for easy visualization: It is expected that any country has 9 measures for each year.

data_suicide1Y = data[data['Indicator'] == "Suicide within 1 year after discharge among patients diagnosed with a mental disorder"]
cross_tab = pd.crosstab( index = data_suicide1Y['Country'], columns=data['Periods'])
cross_tab


# 
# As is evident in the tables, there are missing data:
# 
# Canada, Koreia, Latvia, Lithuania, Malta, Netherlands, Norway, SlovaK Republic, Slovenia, United Kingdom are countries that do not report their data  for more or less then years. We can fill in the data with a linear interpolation, but then we are imposing a behavior of the variables to behave linearly.
# 
# Czech Republic and Colombia  are countries that report their data after 2008. There is so much missed data that an interpolation does not have much sense, for the same reason.
# 
# Chile is a country that starting report the data on period 2008.
# 
# Finland is missing data in the last four years.
# 
# Demark have missing two period of report .
# 
# Iceland, Israel and Sweden no are report any missing periods.
# 
# I opted to eliminate the countries that dont have enough information.
# 
# 
# 

# In[ ]:


#[Write what I am intending to do and why]
# Let's create a series
pd.Series([True, False, True]).sum


# In[ ]:


data['Value'].unique()


# In[ ]:


#[Write what I am intending to do and why]
# Split the 'concatenated_variable' into two different values
temp_df = data['Value'].str.split(', ', 1, expand=True)
temp_df.value_counts()


# ##### Filling Missing Values

# In[ ]:


# value counts: by default drop = True
data["Periods"].value_counts()


# In[ ]:


# value counts: false
data["Periods"].value_counts(dropna=False)


# In[ ]:


data["Periods"].isna().sum()


# In[ ]:


# Fill missing values with a specific value (e.g., 0)
data['Value'].fillna(0, inplace=True)
data['Valuee'].fillna(0, inplace=True)


# In[ ]:


# Drop rows with missing values
data.dropna(subset=['Value', 'Valuee'], inplace=True)


# In[ ]:


# fillna()
data["Periods"].fillna(value="VARIOUS", inplace=True)


# In[ ]:


data.isnull().sum()


# In[ ]:


# now take a look
data["Periods"].value_counts()


# In[ ]:


print(data.columns)


# In[ ]:


# Calculate how the dataframe would change if dropping the missing values
print("Shape: \n", data.dropna().shape, "\n")
print("Count of values: \n", data.dropna().count(), "\n")
print("Missing values: \n", data.dropna().isnull().sum())


# ### Relevance

# #### DROPPING THE DUPLICATE ROWS

# In[ ]:


data.shape


# In[ ]:


duplicate_rows_data = data[data.duplicated()]
print("number of duplicate rows: ", duplicate_rows_data.shape)
# there is no duplicate rows!


# In[ ]:


# Used to count the number of rows
data.count()


# In[ ]:


data.drop_duplicates().shape


# #### DROPPING IRRELEVANT COLUMNS

# In[ ]:


data.columns


# In[ ]:


#Using only the column that I need
dataset = data[["Country","Periods","IND","Indicator","GEN","Gender", "VAL", "Value","Value.1"]]
#Listing the features of the dataset
dataset.columns


# In[ ]:


#[Write about what I am trying to do here]
#dataset = dataset[dataset.GEN != "T"]


# In[ ]:


#[Write about what I am trying to do here]
#dataset['Value'] = pd.to_numeric(data['Value'], errors='coerce') # this causes all values to go NaN
dataset['Value.1'] = pd.to_numeric(data['Value.1'], errors='coerce')


# In[ ]:


# Sort the countries alphabetically into a categorical variable
dataset['Country'] = dataset['Country'].astype('category')
dataset['Country'] = dataset['Country'].cat.reorder_categories(sorted(dataset['Country'].unique()))


# In[ ]:


# Sort the countries by similarity, using the GDP index
my_order = ["Norway","Denmark","Netherlands","Iceland","Sweden","Finland","Canada","Malta","United Kingdom","New Zealand","Korea","Slovenia","Czech Republic","Israel","Lithuania","Latvia","Slovak Republic","Chile","Colombia"]
dataset['Country'].cat.reorder_categories(my_order, inplace= True)


# #### RENAME COLUMNS
# 
# I renamed de column,Value.1 to Valuee because there was string.

# In[ ]:


# rename one of the columns by using `rename` method
#dataset = dataset.rename(columns={'Value.1': 'Value1', 'Age Group': 'AgeeGroup'})  # Age Group contains no information
dataset.rename(columns={'Value.1': 'Valuee'}, inplace=True)
dataset.head()


# In[ ]:





# ##**3. Visualizing the data:**
# 
# Few plots and graphs are displayed to find how the data is distributed and the how features are related to each other.
# 

# In[ ]:


#[ Write about what I am trying to do here...?]
val = []
for countries in dataset.Country.unique():
    val.append(sum(dataset[dataset['Country'] == countries].Valuee)) # Why to sum all the values of each datapoint? they are rates, not units.

val = pd.DataFrame(val, columns = ['Suicide']) # This is not suicide. it is a sum of all the rates, even mortality rates not caused by suicide

countries = pd.DataFrame(dataset.Country.unique(), columns=['country'])

val_suic = pd.concat([val, countries], axis = 1)
print(val_suic)

val_suic = val_suic.sort_values(by = 'Suicide', ascending = False)

fig, ax = plt.subplots(figsize = (12,5))
sns.barplot(y = val_suic.country[:12], x = val_suic.Suicide[:12],color='steelblue')

plt.tight_layout()


# In[ ]:


# Group the data by 'Country' to create separate plots for each country
df = dataset[dataset["IND"]=="EXCEBIPO"]

# Set the style of the plot
sns.set(style="darkgrid")

# Create a line plot using seaborn's relplot
plt.figure(figsize=(8, 10))
sns.relplot(data=df, x='Periods', y='Valuee', hue='Country', kind='line', aspect=2)

# Customize the plot
plt.xlabel('Year')
plt.ylabel('Valuee')
plt.title('Line Plot of Excess Mortality Ratio associated with Bipolar Dissorder Over Time by Country')

# Show the plot
plt.tight_layout()
plt.show()


# In[ ]:


# Group the data by 'Country' to create separate plots for each country
df = dataset[dataset["IND"]=="EXCESCHI"]

# Set the style of the plot
sns.set(style="darkgrid")

# Create a line plot using seaborn's relplot
plt.figure(figsize=(8, 10))
sns.relplot(data=df, x='Periods', y='Valuee', hue='Country', kind='line', aspect=2)

# Customize the plot
plt.xlabel('Year')
plt.ylabel('Valuee')
plt.title('Line Plot of Excess Mortality Ratio associated with Schizophrenia Over Time by Country')

# Show the plot
plt.tight_layout()
plt.show()


# In[ ]:


dataset.head()


# In[ ]:


# Group the data by 'Country' to create separate plots for each country
df = dataset[(dataset["IND"]=="MORTSUMD") & (dataset["VAL"]=="AS_STD_RATE_CPAT")]
print(df.shape)
# Set the style of the plot
sns.set(style="darkgrid")

# Create a line plot using seaborn's relplot
plt.figure(figsize=(8, 10))
sns.relplot(data=df, x='Periods', y='Valuee', hue='Country', kind='line', aspect=2)

# Customize the plot
plt.xlabel('Year')
plt.ylabel('Valuee')
plt.title('Line Plot of Suicide Rate within 1 year of being discharged Over Time by Country')

# Show the plot
plt.tight_layout()
plt.show()


# In[ ]:


# Group the data by 'Country' to create separate plots for each country
df = dataset[(dataset["IND"]=="MORTSUMS") & (dataset["VAL"]=="AS_STD_RATE_CPAT")]
print(df.shape)
# Set the style of the plot
sns.set(style="darkgrid")

# Create a line plot using seaborn's relplot
plt.figure(figsize=(8, 10))
sns.relplot(data=df, x='Periods', y='Valuee', hue='Country', kind='line', aspect=2)

# Customize the plot
plt.xlabel('Year')
plt.ylabel('Valuee')
plt.title('Line Plot of Suicide Rate within 1 month of being discharged Over Time by Country')

# Show the plot
plt.tight_layout()
plt.show()


# ### Israel, the country with most datapoints

# In[ ]:


dataset_i = dataset[dataset.Country == 'Israel'].copy()

dataset_i.head()


# In[ ]:


dataset_i.Indicator.value_counts()


# In[ ]:


#[ Write about what I am trying to do here...?]
fig, ax = plt.subplots(figsize = (11,6))
ax = sns.lineplot(data = dataset_i, x = 'Periods', y = 'Valuee', color = 'b', hue = 'Indicator')

plt.title('Comparison of Suicide Rate and Excess Mortality Ratio in Israel over time')
plt.xlabel('year')
plt.ylabel('percentage')
#plt.xlim(2000, 2022)

plt.tight_layout()


# In[ ]:


#[ Write about what I am trying to do here...?]
fig, ax = plt.subplots(figsize = (10,5))
sns.barplot(data = dataset_i, x = 'Gender', y = 'Valuee', hue = 'Indicator')
plt.title('Comparison of Suicide Rate and Excess Mortality Ratio in Israel against gender')
plt.xlabel('year')
plt.ylabel('Percentage points')

plt.tight_layout()


# In[ ]:


#[ Write about what I am trying to do here...?]
fig, ax = plt.subplots(figsize = (10,5))
sns.barplot(data = dataset_i[dataset_i["IND"].isin(["EXCESCHI", "EXCEBIPO"])], x = 'Gender', y = 'Valuee', hue = 'Indicator')
plt.title('Comparison of Excess Mortality Ratio in Israel against gender')
plt.xlabel('year')
plt.ylabel('Excess Mortality Ratio')
plt.tight_layout()


# In[ ]:


#[ Write about what I am trying to do here...?]
fig, ax = plt.subplots(figsize = (10,5))
sns.barplot(data = dataset_i[dataset_i["IND"].isin(["MORTSUMD", "MORTSUMS"])], x = 'Gender', y = 'Valuee', hue = 'Indicator')
plt.title('Comparison of Suicide Rate in Israel against Gender')
plt.xlabel('year')
plt.ylabel('Rate per 100 patients upper and lower interval')
plt.tight_layout()


# 

# In[ ]:


#[ Write about what I am trying to do here...?]
fig, ax = plt.subplots(figsize = (10,5))
sns.barplot(data = dataset_i[dataset_i["IND"].isin(["MORTSUMD", "MORTSUMS"])], x = 'Gender', y = 'Valuee', hue = 'Value')
plt.title('Comparison of Suicide Rate in Israel against Gender with Upper and Lower confidence intervals')
plt.xlabel('year')
plt.ylabel('Rate per 100 patients upper and lower interval')
plt.tight_layout()


# In[ ]:


dataset_i = dataset_i[dataset_i.GEN != "T"]


# In[ ]:


fig, ax = plt.subplots(figsize = (10,5))

sns.barplot(data = dataset_i, x = 'GEN', y = 'Valuee', hue = 'Indicator', color='steelblue')

plt.tight_layout()


# In[ ]:


fig, ax = plt.subplots(figsize = (10,5))

sns.barplot(data = dataset_i[dataset_i["IND"].isin(["MORTSUMD", "MORTSUMS"]) & (dataset_i["VAL"]=="AS_STD_RATE_CPAT")], x = 'Periods', y = 'Valuee', hue = 'GEN') #color='steelblue' makes some categories almost invisible

plt.tight_layout()


# In[ ]:


dataset = dataset.groupby('GEN')['Valuee'].sum()

plt.pie(dataset, labels = dataset.index,autopct='%.2f%%', shadow=True, colors = ['gray', 'steelblue'], explode=[0.1, 0]);


# In[ ]:


dataset.head()


# ### Pivot the Dataframe to show independent variables

# In[ ]:


# Variables in "IND"/"Indicator" and "VAL'/"Value" have to be separated in different columns?
df = dataset.pivot(index=['Country', 'Periods', 'GEN', 'Gender'], columns=['IND', 'VAL'], values='Valuee').reset_index()
df.head()


# In[ ]:


# Result columns are a multiindex:
df.columns


# In[ ]:


# we can extract the first level of the multiindex:
df.columns.get_level_values(0)


# In[ ]:


#we can flatten the multiindex replacing it for a concatenation (Join) of the different levels of index, for each column. This is done with a list comprehension
df.columns = [' '.join(col).strip() for col in df.columns.values]
df.columns


# In[ ]:


# The msno.matrix() function creates a matrix plot where each column represents a variable, and each row represents an observation (or data point). The white lines in the plot represent missing values. This visualization can help you quickly identify which variables have missing data and whether there are any patterns or clusters of missing values.
# Customize the plot by adding labels, changing the color palette, or adjusting the size to suit your preferences. The missingno library also provides other functions like msno.bar() and msno.heatmap() for different types of missing data visualizations, depending on your needs.
# Create the missingness plot
msno.matrix(df)
plt.show()


# In[ ]:


df2 = df[['Country', 'Periods', 'GEN', 'Gender', 'EXCESCHI AS_STD_RATIO', 'EXCEBIPO AS_STD_RATIO',
         'MORTSUMD AS_STD_RATE_CPAT', # 'MORTSUMD LOW_CI', 'MORTSUMD UP_CI',
         'MORTSUMS AS_STD_RATE_CPAT', # 'MORTSUMS LOW_CI', 'MORTSUMS UP_CI'
         ]]


# In[ ]:


df2 = df2.rename(columns={'Periods':'Year',
                        'EXCESCHI AS_STD_RATIO':'EXCE_SCHI_Std_Ratio',  'EXCEBIPO AS_STD_RATIO':'EXCE_BIPO_Std_Ratio' , #'Excess Mortality Schizophrenia','Excess Mortality Bipolar Disorder'
                        'MORTSUMD AS_STD_RATE_CPAT':'MORT_RATE_SUMD_1Y', 'MORTSUMS AS_STD_RATE_CPAT':'MORT_RATE_SUMS_1M'})


# In[ ]:


# Eliminate Totals in the gender variable, to ease visualization
df2 = df2[df2.GEN != "T"]


# In[ ]:


# Set the style of the plot
sns.set(style="ticks")

# Create a pairplot
sns.pairplot(data=df2, hue='Gender', diag_kind='kde', markers=["o", "s", "D"], palette='husl')

# Customize the plot
plt.suptitle('Pairplot of the Dataset', y=1.02)  # Title at a specific position
plt.show()


# In[ ]:


# Set the style of the plot
sns.set(style="ticks")

# Create a pairplot
sns.pairplot(data=df2, hue='Country', diag_kind='kde', markers=["o", "s", "D"], palette='husl')

# Customize the plot
plt.suptitle('Pairplot of the Dataset', y=1.02)  # Title at a specific position
plt.show()


# In[ ]:


# Visualize correlations between variables
plt.figure(figsize=(10,5))
c= df2.corr()
sns.heatmap(c,cmap="BrBG",annot=True)
c


# In[ ]:


# Calculate the covariance matrix
cov_matrix = df.cov()

# Plot the covariance matrix as a heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(cov_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Covariance Matrix')
plt.show()


# # **Module 2. Predicting Values using Neural Networks**

# ###**Observations:**
# 
# My data has categorical features and text values. And most ML models accept numeric inputs. This is the reason why I will manipulate these types of categories so that they are in the proper format accepted by the ML algorithms.
# 
# 

# In[ ]:


#[ Write what I am trying to do here...?]
data['Result'] = data['Value'] - data['Valuee']


# In[ ]:


map_dict = {
    'Suicide within 30 days after discharge among patients diagnosed with a mental disorder ':0,
    'Suicide within 1 year after discharge among patients diagnosed with a mental disorder': 1,
    'Excess mortality for patients diagnosed with bipolar disorder ': 2
            }


# ### **Encoding**
# 
# Here are four techniques for encoding or converting categorical characteristics into numbers. Here they are:
# 
# Mapping Method
# 
# - Ordinary Encoding
# 
# - Label Encoding
# 
# - Pandas Dummies
# 
# - OneHot Encoding
# 
# Note that some of these encoding techniques may produce the same output, the difference is just the implementation. The first 3 will produce the numeric outputs, while the last one will produce the hot matrix (with 1s and 0s).
# 
# **Categorical Encoding:**
# 
# For categorical variables, you need to encode them into numerical values because neural networks work with numerical data. Common encoding techniques include:
# 
# - One-Hot Encoding: This method creates binary columns for each category in a categorical variable. Each category is represented by a column, and the presence of the category is indicated by a 1 or 0. This is suitable for nominal data (categories without an inherent order).
# - Label Encoding: Label encoding assigns a unique integer to each category. This is suitable for ordinal data (categories with a meaningful order). However, be cautious as neural networks might interpret the encoded integers as having ordinal meaning, which might not be accurate.
# 
# **Scaling Numerical Features:**
# 
# For numerical features, it's important to scale them, especially if they have different ranges. Scaling ensures that all features contribute equally to the model's learning process. Common scaling techniques include:
# 
# - Standardization (Z-score normalization): This scales the features to have a mean of 0 and a standard deviation of 1. It's appropriate when the data follows a normal distribution.
# - Min-Max Scaling: This scales the features to a specific range, often between 0 and 1. It's suitable when you want to preserve the original range of values.
# 
# In our case, Data is already standardised

# #### Ordinary Encoding
# 
# This will also convert categorical data into numbers. Let's implement it

# In[ ]:


#Show me the features of the dataset
data_encoded = dataset.copy()
data_encoded.columns


# In[ ]:


data_filtered =dataset [['Indicator', 'Gender', 'Value']]
data_filtered.head()


# In[ ]:


from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder()

data_codes = encoder.fit_transform(data_filtered)

data_codes


# In[ ]:


data_encoded[['Indicator', 'Gender', 'Value']] = pd.DataFrame(data_codes, columns=data_filtered.columns, index=data_filtered.index)
data_encoded.head()


# In[ ]:


data_encoded.Periods.value_counts()


# In[ ]:


data_encoded.Indicator.value_counts()


# In[ ]:


data_encoded.Gender.value_counts()


# In[ ]:





# In[ ]:


#data_encoded.Valuee.value_counts()


# 

# In[ ]:


df.columns


# ### Create Synthetic Dataset
# 
# The need of the synthetic dataset is to be able to test the models and methods without spending months gathering the required personal data that may be difficult because it is protected by privacy regulations.

# In[ ]:


get_ipython().system('pip install Faker')


# In[ ]:


# Import libraries to create data that is not available for gathering:
from faker import Faker
import datetime
import random
fake = Faker()


# In[ ]:


# Define the number of rows for your dataset
num_rows = 1000

# Create empty lists to store data
patient_ids = []
ages = []
genders = []
diagnoses = []
severity_scores = []
depression_severity = []
diagnosis_dates = []
treatment_history = []
recent_discharge = []
follow_up_period = []
family_history = []
suicide_attempts = []
previous_suicide_attempts = []
lethal_means_access = []
substance_abuse_history = []
current_substance_abuse = []
loss_of_loved_one = []
hopelessness = []
chronic_pain_or_illness = []
medication_changes = []
bullying_discrimination = []
cultural_religious_beliefs = []
social_support = []
social_isolation = []
access_to_healthcare = []
access_to_mental_health_care = []
employment_status = []
financial_status = []
financial_stress = []
geographic_location = []
date_of_suicide = []
suicide_method = []

# Generate synthetic data
for _ in range(num_rows):
    patient_ids.append(fake.unique.random_number(digits=6))
    ages.append(random.randint(18, 80))
    genders.append(random.choice(["Male", "Female", "Non-binary"]))
    diagnoses.append(random.choice(["Depression", "Schizophrenia", "Bipolar Disorder", "Anxiety Disorder"]))
    severity_scores.append(random.uniform(1, 10))
    depression_severity.append(random.uniform(1, 10))
    diagnosis_dates.append(fake.date_of_birth(tzinfo=None, minimum_age=18, maximum_age=80))
    treatment_history.append(fake.paragraph(nb_sentences=3))
    recent_discharge.append(random.choice([True, False]))
    follow_up_period.append(random.randint(30, 365))
    family_history.append(random.choice(["Yes", "No", "Unknown"]))
    suicide_attempts.append(random.choice([True, False]))
    previous_suicide_attempts.append(random.choice([True, False]))
    lethal_means_access.append(random.choice(["Easy", "Limited", "None"]))
    substance_abuse_history.append(random.choice([True, False]))
    current_substance_abuse.append(random.choice([True, False]))
    loss_of_loved_one.append(random.choice([True, False]))
    hopelessness.append(random.choice([True, False]))
    chronic_pain_or_illness.append(random.choice([True, False]))
    medication_changes.append(random.choice([True, False]))
    bullying_discrimination.append(random.choice([True, False]))
    cultural_religious_beliefs.append(random.choice(["Christianity", "Islam", "Buddhism", "Other", "None"]))
    social_support.append(random.choice(["Strong", "Moderate", "Weak"]))
    social_isolation.append(random.choice([True, False]))
    access_to_healthcare.append(random.choice(["High", "Moderate", "Low"]))
    access_to_mental_health_care.append(random.choice(["High", "Moderate", "Low"]))
    employment_status.append(random.choice(["Employed", "Unemployed", "Disability"]))
    financial_status.append(random.randint(1000, 100000))
    financial_stress.append(random.choice([True, False]))
    geographic_location.append(fake.country(unique_countries))
    date_of_suicide.append(fake.date_between(start_date='-1y', end_date='today') if random.choice([True, False]) else None)
    suicide_method.append(random.choice(["Poisoning", "Hanging", "Firearm", "Other"]) if date_of_suicide[-1] is not None else None)


# In[ ]:


# Create a DataFrame
data = {
    'Patient ID': patient_ids,
    'Age': ages,
    'Gender': genders,
    'Diagnosis': diagnoses,
    'Severity of Mental Disorder': severity_scores,
    'Depression Severity': depression_severity,
    'Date of Diagnosis': diagnosis_dates,
    'Treatment History': treatment_history,
    'Recent Discharge from Treatment': recent_discharge,
    'Follow-up Period': follow_up_period,
    'Family History': family_history,
    'Suicide Attempts': suicide_attempts,
    'Previous Suicide Attempts': previous_suicide_attempts,
    'Access to Lethal Means': lethal_means_access,
    'History of Substance Abuse': substance_abuse_history,
    'Current Co-occurring Substance Abuse': current_substance_abuse,
    'Loss of a Loved One': loss_of_loved_one,
    'Hopelessness': hopelessness,
    'Chronic Pain or Illness': chronic_pain_or_illness,
    'Medication Changes': medication_changes,
    'Bullying and Discrimination': bullying_discrimination,
    'Cultural and Religious Beliefs': cultural_religious_beliefs,
    'Social Support': social_support,
    'Social Isolation': social_isolation,
    'Access to Healthcare': access_to_healthcare,
    'Access to Mental Health Care': access_to_mental_health_care,
    'Employment Status': employment_status,
    'Financial Status': financial_status,
    'Financial Stress': financial_stress,
    'Geographic Location': geographic_location,
    'Date of Suicide': date_of_suicide,
    'Suicide Method': suicide_method
}

df = pd.DataFrame(data)

# Save the DataFrame to a CSV file
df.to_csv('HEALTH_SU_Prop_Assess.csv', index=False)


# In[ ]:


df.head()


# ##**Create Multi-Output Regression Model**
# 
# Importanting Data

# In[ ]:


import pandas as pd
from sklearn.datasets import make_regression
from keras.models import Sequential
from keras.layers import Dense


# In[ ]:


def get_dataset():
  # Create sample data with sklearn make_regression function
  X, y = make_regression(n_samples=1000, n_features=10, n_informative=7, n_targets=5, random_state=0)

  # Convert the data into Pandas Dataframes for easier maniplution and keeping stored column names
  # Create feature column names
  feature_cols = ['feature_01', 'feature_02', 'feature_03', 'feature_04',
                  'feature_05', 'feature_06', 'feature_07', 'feature_08',
                  'feature_09', 'feature_10']

  df_features = pd.DataFrame(data = X, columns = feature_cols)

  # Create lable column names and dataframe
  label_cols = ['labels_01', 'labels_02', 'labels_03', 'labels_04', 'labels_05']

  df_labels = pd.DataFrame(data = y, columns = label_cols)

  return df_features, df_labels


# ###**Create Model**
# 
# Create a Tensorflow/Keras Sequential model

# In[ ]:


def get_model(n_inputs, n_outputs):
    model = Sequential()
    model.add(Dense(32, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))
    model.add(Dense(n_outputs, kernel_initializer='he_uniform'))
    model.compile(loss='mae', optimizer='adam')
    return model


# ###**Train Model**

# In[ ]:


# Create the datasets
X, y = get_dataset()

# Get the number of inputs and outputs from the dataset
n_inputs, n_outputs = X.shape[1], y.shape[1]


# Load the model with function

# In[ ]:


model = get_model(n_inputs, n_outputs)


# Train the model

# In[ ]:


model.fit(X, y, verbose=0, epochs=100)


# Get model evaluation metrics to confirm training went well

# In[ ]:


model.evaluate(x = X, y = y)


# ###**Model Prediction**

# In[ ]:


model.predict(X.iloc[0:1,:])


# ###**Get SHAP Values and Plots**

# In[ ]:


get_ipython().system('pip install shap')
import shap

# print the JS visualization code to the notebook
shap.initjs()


# Here I take the Keras model trained above and explain why it makes different predictions on individual samples

# In[ ]:


explainer = shap.KernelExplainer(model = model.predict, data = X.head(50), link = "identity")


# Get the Shapley value

# In[ ]:


# Set the index of the specific example to explain
X_idx = 0

shap_value_single = explainer.shap_values(X = X.iloc[X_idx:X_idx+1,:], nsamples = 100)


# Display the details

# In[ ]:


X.iloc[X_idx:X_idx+1,:]


# Choosing the label/output/destination to run individual explanations

# In[ ]:


import ipywidgets as widgets


# In[ ]:


# Create the list of all labels for the drop down list
list_of_labels = y.columns.to_list()

# Create a list of tuples so that the index of the label is what is returned
tuple_of_labels = list(zip(list_of_labels, range(len(list_of_labels))))

# Create a widget for the labels and then display the widget
current_label = widgets.Dropdown(options=tuple_of_labels,
                              value=0,
                              description='Select Label:'
                              )

# Display the dropdown list (Note: access index value with 'current_label.value')
current_label


# Plot the force plot for a single example and a single label/output/target

# In[ ]:


# print the JS visualization code to the notebook
shap.initjs()

print(f'Current label Shown: {list_of_labels[current_label.value]}')

shap.force_plot(base_value = explainer.expected_value[current_label.value],
                shap_values = shap_value_single[current_label.value],
                features = X.iloc[X_idx:X_idx+1,:]
                )


# Create the summary plot for a specific output/label/target

# In[ ]:


# Note: I are limiting to the first 50 training examples since it takes time to calculate the full number of sampels
shap_values = explainer.shap_values(X = X.iloc[0:50,:], nsamples = 100)


# In[ ]:


# print the JS visualization code to the notebook
shap.initjs()

print(f'Current Label Shown: {list_of_labels[current_label.value]}\n')

shap.summary_plot(shap_values = shap_values[current_label.value],
                  features = X.iloc[0:50,:]
                  )


# Looking at the summary graph above, we can see that features 01, 07,03 and 04 are the features that have no influence on the model and can be discarded (note that in the data configuration we chose 10 features and only 6 of them had a significant impact) . Then we can see which features are actually influencing our model.

# In[ ]:


print(f'Current Label Shown: {list_of_labels[current_label.value]}\n')

# print the JS visualization code to the notebook
shap.initjs()

shap.force_plot(base_value = explainer.expected_value[current_label.value],
                shap_values = shap_values[current_label.value],
                features = X.iloc[0:50,:])


# ## **Sentiment Analysis with Logistic Regression**

# In[ ]:


import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np
import shap
np.random.seed(101)
shap.initjs()


# In[ ]:


corpus,y = shap.datasets.imdb()
corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.2, random_state=7)

vectorizer = TfidfVectorizer(min_df=10)
X_train = vectorizer.fit_transform(corpus_train).toarray() # sparse also works but Explanation slicing is not yet supported
X_test = vectorizer.transform(corpus_test).toarray()


# In[ ]:


model = sklearn.linear_model.LogisticRegression(penalty="l2", C=0.1)
model.fit(X_train, y_train)
print(classification_report(y_test, model.predict(X_test)))


# In[ ]:


explainer = shap.Explainer(model, X_train, feature_names=vectorizer.get_feature_names_out())
shap_values = explainer(X_test)


# In[ ]:


shap.plots.beeswarm(shap_values)


# In[ ]:


ind = 0
shap.plots.force(shap_values[ind])


# In[ ]:


print("Positive" if y_test[ind] else "Negative", "Review:")
print(corpus_test[ind])


# In[ ]:


ind = 1
shap.plots.force(shap_values[ind])


# In[ ]:


print("Positive" if y_test[ind] else "Negative", "Review:")
print(corpus_test[ind])


# In[ ]:


ind = 2
shap.plots.force(shap_values[ind])


# In[ ]:


print("Positive" if y_test[ind] else "Negative", "Review:")
print(corpus_test[ind])


# ## **Build a linear model that uses standardized features**
# 
# 
# A linear model that uses standardized features is a statistical model in which the independent variables (features) have been transformed or standardized to have a mean of 0 and a standard deviation of 1. Standardization involves subtracting the mean value of each feature from all data points and then dividing by the standard deviation. This process ensures that all features have the same scale and allows the linear model to give equal importance to each feature during training. Standardization is often performed to improve the performance and interpretability of linear models, such as linear regression.

# In[ ]:


import sklearn
import shap

# get standardized data
X, y = shap.datasets.california()
scaler = sklearn.preprocessing.StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)

# train the linear model
model = sklearn.linear_model.LinearRegression().fit(X_std, y)

# explain the model's predictions using SHAP
explainer = shap.explainers.Linear(model, X_std)
shap_values = explainer(X_std)

# visualize the model's dependence on the first feature
shap.plots.scatter(shap_values[:, 0])


# In[ ]:


# we add back the feature names stripped by the StandardScaler
for i,c in enumerate(X.columns):
    shap_values.feature_names[i] = c

# we convert back to the original data
# (note we can do this because X_std is a set of univariate transformations of X)
shap_values.data = X.values

# visualize the model's dependence on the first feature again, now in the new original feature space
shap.plots.scatter(shap_values[:, 0])


# ## **Permutation Explainer**
# 
# A Permutation Explainer is an interpretability technique used in machine learning to explain the importance or impact of individual features on a model's predictions. It works by systematically permuting (shuffling) the values of a specific feature while keeping other features constant and observing how the model's predictions change. The extent to which the predictions are affected by the permutation of a feature indicates its importance: if the predictions change significantly, the feature is considered important, while minimal change suggests lower importance. Permutation explainers are often used for feature importance analysis and model interpretation.
# 
# 
# 
# 
# 
# 

# Demonstrating how to use the permutation explainer on a dataset and simple classification model

# In[ ]:


import xgboost

import shap

# get a dataset on income prediction
X, y = shap.datasets.adult()

# train an XGBoost model (but any other model type would also work)
model = xgboost.XGBClassifier()
model.fit(X, y);


# Tabular data with independent (Shapley value)

# In[ ]:


# build a Permutation explainer and explain the model predictions on the given dataset
explainer = shap.explainers.Permutation(model.predict_proba, X)
shap_values = explainer(X[:100])

# get just the explanations for the positive class
shap_values = shap_values[..., 1]


# Plot a global summary

# In[ ]:


shap.plots.bar(shap_values)


# Plot a single instance

# In[ ]:


shap.plots.waterfall(shap_values[0])


# Tabular data with partition (Owen value) masking
# 
# Tabular data with partition (Owen value) masking is a method used to protect sensitive information in structured datasets by dividing the data into partitions and adding noise to the values within each partition, balancing data analysis and privacy.

# In[ ]:


# build a clustering of the features based on shared information about y
clustering = shap.utils.hclust(X, y)


# Plot a global summary

# In[ ]:


# above we implicitly used shap.maskers.Independent by passing a raw dataframe as the masker
# now we explicitly use a Partition masker that uses the clustering we just computed
masker = shap.maskers.Partition(X, clustering=clustering)

# build a Permutation explainer and explain the model predictions on the given dataset
explainer = shap.explainers.Permutation(model.predict_proba, masker)
shap_values2 = explainer(X[:100])

# get just the explanations for the positive class
shap_values2 = shap_values2[..., 1]


# In[ ]:


shap.plots.bar(shap_values2)


# Plot a single instance

# In[ ]:


shap.plots.waterfall(shap_values2[0])


# ##**The supervised machine learning models (regression) considered to train the dataset in this notebook are:**
# 
# * k-Nearest Neighbors Regression
# 
# * Linear Regression
# 
# * Decision Tree
# 
# * Random Forest
# 
# * Gradient Boosting
# 
# * Multilayer Perceptrons
# 
# * XGBoost
# 
# * Bagging Regression
# 
# * Custom Ensemble: SuperLearner

# In[ ]:


#importing required libraries
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV


# In[ ]:


# Creating holders to store the model performance results
ML_Model = []
acc_train = []
acc_test = []
rmse_train = []
rmse_test = []

#function to call for storing the results
def storeResults(model, a,b,c,d):
    ML_Model.append(model)
    acc_train.append(round(a, 3))
    acc_test.append(round(b, 3))
    rmse_train.append(round(c, 3))
    rmse_test.append(round(d, 3))


# ##**k-Nearest Neighbors Regression:**
# 
# K nearest neighbors is a simple algorithm that stores all available cases and predict the numerical target based on a similarity measure (e.g., distance functions). A simple implementation of KNN regression is to calculate the average of the numerical target of the k nearest neighbors.

# In[ ]:


from sklearn.neighbors import KNeighborsRegressor

# instantiate the model
knn = KNeighborsRegressor()

param_grid = {'n_neighbors':list(range(1, 31)), 'weights': ['uniform', 'distance']}

# instantiate the grid
knn_grid = GridSearchCV(knn, param_grid , cv=10)

# fit the model
knn_grid.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_train_knn = knn_grid.predict(X_train)
y_test_knn = knn_grid.predict(X_test)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_knn = knn_grid.score(X_train, y_train)
acc_test_knn = knn_grid.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_knn = np.sqrt(mean_squared_error(y_train, y_train_knn))
rmse_test_knn = np.sqrt(mean_squared_error(y_test, y_test_knn))

print("KNN: Accuracy on training Data: {:.3f}".format(acc_train_knn))
print("KNN: Accuracy on test Data: {:.3f}".format(acc_test_knn))
print('\nKNN: The RMSE of the training set is:', rmse_train_knn)
print('KNN: The RMSE of the testing set is:', rmse_test_knn)


# Reserving Results:

# In[ ]:


#storing the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('k-Nearest Neighbors Regression', acc_train_knn, acc_test_knn, rmse_train_knn, rmse_test_knn)


# Evaluating training and testing set performance with different numbers of neighbors from 1 to 30. The plot shows the training and test set accuracy on the y-axis against the setting of n_neighbors on the x-axis.

# In[ ]:


training_accuracy = []
test_accuracy = []
# try n_neighbors from 1 to 20
neighbors_settings = range(1, 31)
for n in neighbors_settings:
    # fit the model
    knn = KNeighborsRegressor(n_neighbors=n)
    knn.fit(X_train, y_train)
    # record training set accuracy
    training_accuracy.append(knn.score(X_train, y_train))
    # record generalization accuracy
    test_accuracy.append(knn.score(X_test, y_test))

#plotting the training & testing accuracy for n_neighbours from 1 to 30
plt.plot(neighbors_settings, training_accuracy, label="training accuracy")
plt.plot(neighbors_settings, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()


# OBSERVATIONS: This discrepancy between performance on the training set and the testing set from n_neighbors is a clear sign of overfitting. After that, the perfromance is deprecated, moving on to the other models.

# ##**Linear Regression**
#  Linear regression, or ordinary least squares (OLS), is the simplest and most classic linear method for regression. Linear regression finds the parameters w and b that minimize the mean squared error between predictions and the true regression targets, y, on the training set.

# In[ ]:


# Linear regression model
from sklearn.linear_model import LinearRegression

# instantiate the model
lr = LinearRegression()
# fit the model
lr.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_test_lr = lr.predict(X_test)
y_train_lr = lr.predict(X_train)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_lr = lr.score(X_train, y_train)
acc_test_lr = lr.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_lr = np.sqrt(mean_squared_error(y_train, y_train_lr))
rmse_test_lr = np.sqrt(mean_squared_error(y_test, y_test_lr))

print("Linear Regression: Accuracy on training Data: {:.3f}".format(acc_train_lr))
print("Linear Regression: Accuracy on test Data: {:.3f}".format(acc_test_lr))
print('\nLinear Regression: The RMSE of the training set is:', rmse_train_lr)
print('Linear Regression: The RMSE of the testing set is:', rmse_test_lr)


# Reserving Results:

# In[ ]:


#storing the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('Linear Regression', acc_train_lr, acc_test_lr, rmse_train_lr, rmse_test_lr)


# OBSERVATIONS: The model's performance is not one of the best, but we can see that the scores on the training and testing sets are very close. This means we are probably underfitting rather than overfitting.

# ##**Decision Trees: Regression**
# 
# Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision. Learning a decision tree means learning the sequence of if/else questions that gets us to the true answer most quickly.
# 
#  In the machine learning setting, these questions are called tests (not to be confused with the test set, which is the data we use to test to see how generalizable our model is). To build a tree, the algorithm searches over all possible tests and finds the one that is most informative about the target variable.

# In[ ]:


# Decision Tree regression model
from sklearn.tree import DecisionTreeRegressor

# instantiate the model
tree = DecisionTreeRegressor(max_depth=9)
# fit the model
tree.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_test_tree = tree.predict(X_test)
y_train_tree = tree.predict(X_train)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_tree = tree.score(X_train, y_train)
acc_test_tree = tree.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_tree = np.sqrt(mean_squared_error(y_train, y_train_tree))
rmse_test_tree = np.sqrt(mean_squared_error(y_test, y_test_tree))

print("Decision Tree: Accuracy on training Data: {:.3f}".format(acc_train_tree))
print("Decision Tree: Accuracy on test Data: {:.3f}".format(acc_test_tree))
print('\nDecision Tree: The RMSE of the training set is:', rmse_train_tree)
print('Decision Tree: The RMSE of the testing set is:', rmse_test_tree)


# Reserving Results:

# In[ ]:


#storing the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('Decision Tree',acc_train_tree, acc_test_tree, rmse_train_tree, rmse_test_tree)


# In[ ]:


#checking the feature improtance in the model
plt.figure(figsize=(9,7))
n_features = X_train.shape[1]
plt.barh(range(n_features), tree.feature_importances_, align='center')
plt.yticks(np.arange(n_features), X_train.columns)
plt.xlabel("Feature importance")
plt.ylabel("Feature")
plt.show()


# Evaluating training and testing set performance with different numbers of max_depth from sex 0.00 to S5 0.40. The plot shows the training and test set accuracy on the y-axis against the setting of max_depth on the x-axis.

# In[ ]:


training_accuracy = []
test_accuracy = []
# try max_depth from 1 to 30
depth = range(1, 31)
for n in depth:
    # fit the model
    tree = DecisionTreeRegressor(max_depth=n)
    tree.fit(X_train, y_train)
    # record training set accuracy
    training_accuracy.append(tree.score(X_train, y_train))
    # record generalization accuracy
    test_accuracy.append(tree.score(X_test, y_test))

#plotting the training & testing accuracy for max_depth from 1 to 30
plt.plot(depth, training_accuracy, label="training accuracy")
plt.plot(depth, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("max_depth")
plt.legend()


# OBSERVATIONS: The model preformance is gradually increased on decrease the max_depth parameter. But after max_depth = 9, the model overfits. So the model is considered with max_depth = 8 which has an accuracy of 95.2%.

#  ## **Random Forest: Ensemble of Decision Trees**
# 
# Random forests for regression and classification are currently among the most widely used machine learning methods.A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data.
# 
#  If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. To build a random forest model, you need to decide on the number of trees to build (the n_estimators parameter of RandomForestRegressor or RandomForestClassifier). They are very powerful, often work well without heavy tuning of the parameters, and donâ€™t require scaling of the data.

# In[ ]:


# Random Forest regression model
from sklearn.ensemble import RandomForestRegressor

# instantiate the model
forest = RandomForestRegressor(max_depth=9)

# fit the model
forest.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_test_forest = forest.predict(X_test)
y_train_forest = forest.predict(X_train)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_forest = forest.score(X_train, y_train)
acc_test_forest = forest.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_forest = np.sqrt(mean_squared_error(y_train, y_train_forest))
rmse_test_forest = np.sqrt(mean_squared_error(y_test, y_test_forest))

print("Random Forest: Accuracy on training Data: {:.3f}".format(acc_train_forest))
print("Random Forest: Accuracy on test Data: {:.3f}".format(acc_test_forest))
print('\nRandom Forest: The RMSE of the training set is: ', rmse_train_forest)
print('Random Forest: The RMSE of the testing set is: ', rmse_test_forest)


# Reserving Results:

# In[ ]:


#storing the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('Random Forest',acc_train_forest, acc_test_forest, rmse_train_forest, rmse_test_forest)


# Evaluating training and testing set performance with different numbers of max_depth from 1 to 30. The plot shows the training and test set accuracy on the y-axis against the setting of max_depth on the x-axis.

# In[ ]:


training_accuracy = []
test_accuracy = []
# try max_depth from 1 to 30
depth = range(1, 31)
for n in depth:
    # fit the model
    forest = RandomForestRegressor(max_depth=n)
    forest.fit(X_train, y_train)
    # record training set accuracy
    training_accuracy.append(forest.score(X_train, y_train))
    # record generalization accuracy
    test_accuracy.append(forest.score(X_test, y_test))

#plotting the training & testing accuracy for max_depth from 1 to 30
plt.plot(depth, training_accuracy, label="training accuracy")
plt.plot(depth, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("max_depth")
plt.legend()


#  ## **Multilayer Perceptrons (MLPs): Deep Learning**
# 
#    Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neural networks. Multilayer perceptrons can be applied for both classification and regression problems.
# 
#    MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.

# In[ ]:


# Multilayer Perceptrons model
from sklearn.neural_network import MLPRegressor

# instantiate the model
mlp = MLPRegressor(hidden_layer_sizes=([100,100]))

# fit the model
mlp.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_test_mlp = mlp.predict(X_test)
y_train_mlp = mlp.predict(X_train)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_mlp = mlp.score(X_train, y_train)
acc_test_mlp = mlp.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_mlp = np.sqrt(mean_squared_error(y_train, y_train_mlp))
rmse_test_mlp = np.sqrt(mean_squared_error(y_test, y_test_mlp))

print("Multilayer Perceptron Regression: Accuracy on training Data: {:.3f}".format(acc_train_mlp))
print("Multilayer Perceptron Regression: Accuracy on test Data: {:.3f}".format(acc_test_mlp))
print('\nMultilayer Perceptron Regression: The RMSE of the training set is: ', rmse_train_mlp)
print('Multilayer Perceptron Regression: The RMSE of the testing set is: ', rmse_test_mlp)


# Reserving Results:

# In[ ]:


#Reserving the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('Multilayer Perceptron Regression',acc_train_mlp, acc_test_mlp, rmse_train_mlp, rmse_test_mlp)


# Observations: The model didnt overfit when trained without tuning any parameters. But, the model accuracy obtained is 89.2%.
# 
# So, hyperparameter tuning is performed for the model. The tuned parameters are number of hidden layers and the hidden_units of each layer with default values of alpha. The otimized Gradient Boosted model gives us an accuracy of 92.8%, with parameter tuning.

# ## **XGBoost Regression:**
# 
#  XGBoost is one of the most popular machine learning algorithms these days. XGBoost stands for eXtreme Gradient Boosting. Regardless of the type of prediction task at hand; regression or classification. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.

# In[ ]:


#XGBoost Regression model
from xgboost import XGBRegressor

# instantiate the model
xgb = XGBRegressor(learning_rate=0.2,max_depth=4)
#fit the model
xgb.fit(X_train, y_train)


# In[ ]:


#predicting the target value from the model for the samples
y_test_xgb = xgb.predict(X_test)
y_train_xgb = xgb.predict(X_train)


# Performance Evaluation:

# In[ ]:


from sklearn.metrics import mean_squared_error
import numpy as np

# Assuming you have predicted values for the training and test datasets
y_train_predicted_xgb = xgb_model.predict(X_train)
y_test_predicted_xgb = xgb_model.predict(X_test)

# Calculate RMSE for training and test sets
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_predicted_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_predicted_xgb))

# Print the RMSE values
print("XGBoost Regression: RMSE on training Data: {:.3f}".format(rmse_train_xgb))
print("XGBoost Regression: RMSE on test Data: {:.3f}".format(rmse_test_xgb))


# In[ ]:


#computing the accuracy of the model performance
acc_train_xgb = xgb.score(X_train, y_train)
acc_test_xgb = xgb.score(X_test, y_test)

#computing root mean squared error (RMSE)
rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_xgb))
rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_xgb))

print("XGBoost Regression: Accuracy on training Data: {:.3f}".format(acc_train_xgb))
print("XGBoost Regression: Accuracy on test Data: {:.3f}".format(acc_test_xgb))
print('\nXGBoost Regression: The RMSE of the training set is: ', rmse_train_xgb)
print('XGBoost Regression: The RMSE of the testing set is: ', rmse_test_xgb)


# In[ ]:


#Reserving the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('XGBoost Regression',acc_train_xgb, acc_test_xgb, rmse_train_xgb, rmse_test_xgb)


# Observations: aaaaa

# ## **Custom Ensemble - SuperLearner:**
# 
# To build a custom ensemble, a Python library called mlens is used. mlens is short of ML-Ensemble used for memory efficient parallelized ensemble learning. ML-Ensemble is a library for building Scikit-learn compatible ensemble estimator. Ensembles are built as a feed-forward network, with a set of layers stacked on each other.

# In[ ]:


### You will need mlens package
get_ipython().system('pip install mlens')


# In[ ]:


# Load a sample regression dataset (Diabetes dataset)
data = load_diabetes()
X, y = data.data, data.target


# In[ ]:


# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# In[ ]:


# Define the meta-model (the model that combines predictions)
meta_model = LinearRegression()


# In[ ]:


# Create the stacking regressor
stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)


# In[ ]:


# Fit the stacking model to the training data
stacking_model.fit(X_train, y_train)


# In[ ]:


# Make predictions on the test set
y_pred = stacking_model.predict(X_test)


# In[ ]:


# Calculate the RMSE (Root Mean Squared Error) to evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Stacking Regressor RMSE:", rmse)


# In[ ]:


from mlens.ensemble import SuperLearner
from mlens.model_selection import Evaluator
from mlens.metrics import make_scorer
from mlens.metrics.metrics import rmse

from sklearn.metrics import accuracy_score


# In[ ]:


# --- Build ---
# Passing a scoring function will create cv scores during fitting
#the scorer should be a simple function accepting to vectors and returning a scalar
ensemble = SuperLearner(scorer=rmse, random_state=555, verbose=2)

# Build the first layer
ensemble.add(mlp)
ensemble.add(knn_grid)
ensemble.add_meta(lr)


# In[ ]:


# Fit ensemble
ensemble.fit(X_train, y_train)


# In[ ]:


#Predicting the target of samples from the model
y_train_en = ensemble.predict(X_train)
y_test_en = ensemble.predict(X_test)


# Performance Evaluation:

# In[ ]:


#computing the accuracy of the model performance
acc_train_en = sklearn.metrics.r2_score(y_train,y_train_en)
acc_test_en = sklearn.metrics.r2_score(y_test,y_test_en)

#computing root mean squared error (RMSE)
rmse_train_en = rmse(y_train,y_train_en)
rmse_test_en = rmse(y_test,y_test_en)

print("Custom Ensemble: Accuracy on training Data: {:.3f}".format(acc_train_en))
print("Custom Ensemble: Accuracy on test Data: {:.3f}".format(acc_test_en))
print('\nCustom Ensemble: The RMSE of the training set is: ', rmse_train_en)
print('Custom Ensemble: The RMSE of the testing set is: ', rmse_test_en)


# Reserving Results:

# In[ ]:


#storing the results. The below mentioned order of parameter passing is important.
#Caution: Execute only once to avoid duplications.
storeResults('Ensemble_SuperLearner',acc_train_en, acc_test_en, rmse_train_en, rmse_test_en)


# OBSERVATIONS:

# # Debate the theory and application of different types of neural networks.

# ##**Comparision of Models:**
# 
# To compare the models performance, a dataframe is created. The columns of this dataframe are the lists created to store the results of the model.

# In[ ]:


#creating dataframe
results = pd.DataFrame({ 'ML Model': ML_Model,
    'Train Accuracy': acc_train,
    'Test Accuracy': acc_test,
    'Train RMSE': rmse_train,
    'Test RMSE': rmse_test})


# In[ ]:


results


# In[ ]:


#Sorting the datafram on accuracy
results.sort_values(by=['Test Accuracy', 'Train Accuracy'], ascending=False)


# ##**Statistical Tests:**
# 
# Statistical tests are used in hypothesis testing. They can be used to:  determine whether a predictor variable has a statistically significant relationship with an outcome variable.
# estimate the difference between two or more groups
# 
# 

# In[ ]:


#improting required libraries
from scipy import stats


# **Teste 1:** Sample t-test to check the difference in suicide rates between
# male and female. The hypothesis statements for this test are:
# 
# * H0: There is no difference in the suicide rates among male and female (Null).
# 
# * H1: There is difference in the suicide rates among male and female (Alternate)

# In[ ]:


#collecting male suicide rate data
male = stat_data['suicide_rate'][stat_data['gender'] == 1]
male


# In[ ]:


#collecting female suicide rate data
female = stat_data['suicide_rate'][stat_data['gender'] == 0]
female


# In[ ]:


#calculating p value
ttest,pval = stats.ttest_rel(male, female)

if pval<0.05:
    print("Reject null hypothesis")
else:
    print("Accept null hypothesis")


# Test Conclusion:

# **Test2:** Finding out whether there is a dependence of suicide rate on the age using the Chi- Square test. The hypothesis statements for this test are:
# 
# * H0: Suicide rate and age are independent (Null).
# 
# * H1: Suicide rate and age are dependent (Alternate).

# In[ ]:


#Creating Contingency Table
contingency_table = pd.crosstab(stat_data.suicide_rate, stat_data.age_group)


# In[ ]:


#Significance Level 5%
alpha=0.05


# In[ ]:


chistat, p, dof, expected = stats.chi2_contingency(contingency_table )


# In[ ]:


#critical_value
critical_value=stats.chi2.ppf(q=1-alpha,df=dof)
print('critical_value:',critical_value)


# In[ ]:


print('Significance level: ',alpha)
print('Degree of Freedom: ',dof)
print('chi-square statistic:',chistat)
print('critical_value:',critical_value)
print('p-value:',p)
#Here, pvalue = 0.0 and a low pvalue suggests that your sample provides enough evidence that you can reject  H0  for the entire population.


# In[ ]:


#compare chi_square_statistic with critical_value and p-value which is the
#probability of getting chi-square>0.09 (chi_square_statistic)
if chistat>=critical_value:
   print("Reject H0,There is a dependency between Age group & Suicide rate.")
else:
   print("Retain H0,There is no relationship between Age group & Suicide rate.")

if p<=alpha:
   print("Reject H0,There is a dependency between Age group & Suicide rate.")
else:
   print("Retain H0,There is no relationship between Age group & Suicide rate.")


# ##**Final Conclusion:**
# In this project, learn how to work with different machine learning models on a set of data and understand their parameters. Creating this notebook helped me learn a lot about model parameters, how to tune them, and how they affect model performance. The final conclusion from the suicide dataset is that, regardless of age group and generation, the male population in each country is more likely to commit suicide than the female population.
